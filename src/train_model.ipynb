{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "import torch\n",
    "import torch.version\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.version.__version__\n",
    "print('PyTorch Version:',torch.version.__version__)\n",
    "print('Cuda Version:',torch.version.cuda,'\\n')\n",
    "\n",
    "print('Available devices:')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print('\\t',torch.cuda.get_device_properties(i).name)\n",
    "   print('\\t\\tMultiprocessor Count:',torch.cuda.get_device_properties(i).multi_processor_count)\n",
    "   print('\\t\\tTotal Memory:',torch.cuda.get_device_properties(i).total_memory/1024/1024, 'MB')\n",
    "   \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('\\n',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage import transform\n",
    "import pickle\n",
    "from data_tools import *\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = '../datasets/VTNet/'\n",
    "\n",
    "with open(f'{DATASETS_DIR}trainset_vtnet_{2}.pkl', 'rb') as file:\n",
    "    train_set = pickle.load(file)\n",
    "\n",
    "with open(f'{DATASETS_DIR}valset_vtnet_{2}.pkl', 'rb') as file:\n",
    "    val_set = pickle.load(file)\n",
    "\n",
    "with open(f'{DATASETS_DIR}testset_vtnet_{2}.pkl', 'rb') as file:\n",
    "    test_set = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VETNet(nn.Module):\n",
    "    def __init__(self, timeseries_size ,scanpath_size, cnn_shape = (16,6)):\n",
    "        super(VETNet, self).__init__()\n",
    "        self.scanpath_layer = nn.Sequential(\n",
    "            nn.Conv2d(scanpath_size[0],cnn_shape[0],kernel_size=5,padding='same'),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(cnn_shape[0],cnn_shape[1],kernel_size=5,padding='same'),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(scanpath_size[1]*scanpath_size[2]*cnn_shape[1]//16,50)\n",
    "        )\n",
    "        \n",
    "        self.timeseries_layer_attention = nn.MultiheadAttention(timeseries_size[-1]-1,1, batch_first=True)\n",
    "        self.timeseries_layer_gru = nn.GRU(input_size=timeseries_size[-1]-1, hidden_size=256, batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(306,20),\n",
    "            #nn.LeakyReLU(),\n",
    "            nn.Linear(20,2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_timeseries, x_scanpath):\n",
    "        x_scanpath = self.scanpath_layer(x_scanpath)\n",
    "        \n",
    "        x_timeseries,_ = self.timeseries_layer_attention(x_timeseries,x_timeseries,x_timeseries)\n",
    "        _ ,x_timeseries = self.timeseries_layer_gru(x_timeseries)\n",
    "        \n",
    "        x = torch.cat((torch.squeeze(x_timeseries,0),x_scanpath),dim=1)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience: int, mode: str, minimum_delta = 0.0):\n",
    "        assert mode in {'min', 'max'}, \"mode has to be 'min' or 'max'\"\n",
    "        self.minimum_delta = minimum_delta\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.tracking = torch.inf if mode == 'min' else -torch.inf\n",
    "        self.mode = 1 if mode=='max' else -1\n",
    "        self.best_model = None\n",
    "        \n",
    "    def check(self, value, model: nn.Module):\n",
    "        if self.mode*(value-self.tracking) <= self.minimum_delta:\n",
    "            self.counter+=1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            self.tracking = value\n",
    "            self.best_model = deepcopy(model.state_dict())\n",
    "            \n",
    "        if self.counter == self.patience:\n",
    "            model.load_state_dict(self.best_model)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "class ReduceLROnPlateau():\n",
    "    def __init__(self, patience: int, rate: float, mode: str, minimum_lr = 0.0, minimum_delta = 0.0):\n",
    "        assert rate<=1 and rate>0, \"rate as to be a number between 0 and 1\"\n",
    "        assert mode in {'min', 'max'}, \"mode has to be 'min' or 'max'\"\n",
    "\n",
    "        self.minimum_delta = minimum_delta\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.rate = rate\n",
    "        self.minimum_lr = minimum_lr\n",
    "        self.tracking = torch.inf if mode == 'min' else -torch.inf\n",
    "        self.mode = 1 if mode=='max' else -1\n",
    "        self.best_model = None\n",
    "        \n",
    "    def check(self, value, optimizer, model):\n",
    "        if self.mode*(value-self.tracking) <= self.minimum_delta:\n",
    "            self.counter+=1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            self.tracking = value\n",
    "            self.best_model = deepcopy(model.state_dict())\n",
    "            \n",
    "        if self.counter == self.patience:\n",
    "            for i in range(len(optimizer.param_groups)):\n",
    "                optimizer.param_groups[i]['lr'] = max(self.rate*optimizer.param_groups[i]['lr'], self.minimum_lr)\n",
    "            model.load_state_dict(self.best_model)\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VETNet(timeseries_size=train_set[0][0].shape, scanpath_size=train_set[0][1].shape).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "lr_tracker = ReduceLROnPlateau(5, 0.5, mode='min', minimum_lr=1e-6)\n",
    "earlystop_tracker = EarlyStopping(10, mode='min')\n",
    "\n",
    "trainloader = DataLoader(train_set, batch_size=batchsize, shuffle=True)\n",
    "valloader = DataLoader(val_set, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = []\n",
    "val_running_loss = []\n",
    "for epoch in range(1,101):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss += [0.0]\n",
    "    val_running_loss += [0.0]\n",
    "    \n",
    "    with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "        for input_rawdata, input_scanpath, labels in tepoch:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_rawdata = input_rawdata[:,:,1:].to(device)\n",
    "            input_scanpath = (input_scanpath/128-1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(input_rawdata, input_scanpath)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss[-1] += loss.item()\n",
    "            \n",
    "            tepoch.set_postfix(loss=running_loss[-1])\n",
    "    \n",
    "        for val_rawdata, val_scanpath, val_labels in valloader:\n",
    "            val_rawdata = val_rawdata.to(device)\n",
    "            val_scanpath = (val_scanpath/128-1).to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            \n",
    "            val_outputs = model(val_rawdata[:,:,1:], val_scanpath)\n",
    "            val_loss = val_criterion(val_outputs, val_labels)\n",
    "            val_running_loss[-1] += val_loss.item()\n",
    "        \n",
    "        print(f\"\\t Training Loss (final): {running_loss[-1]/len(train_set): .4f}, Validation Loss: {val_running_loss[-1]/len(val_set): .4f}, Learning Rate: {optimizer.param_groups[-1]['lr']: .2E}\")\n",
    "        \n",
    "        lr_tracker.check(value=val_running_loss[-1], optimizer=optimizer, model=model)\n",
    "        \n",
    "        if earlystop_tracker.check(value=val_running_loss[-1], model=model):\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=batchsize, shuffle=False)\n",
    "classes = ['CONTROL', 'PATIENT']\n",
    "\n",
    "with torch.no_grad():\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    \n",
    "    for input_rawdata, input_scanpath, labels in test_loader:\n",
    "        \n",
    "        input_rawdata = input_rawdata[:,:,1:].to(device)\n",
    "        input_scanpath = (input_scanpath/128-1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_rawdata, input_scanpath)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        TP += torch.sum((predicted==0)[labels==0])\n",
    "        FN += torch.sum((predicted==1)[labels==0])\n",
    "        FP += torch.sum((predicted==0)[labels==1])\n",
    "        TN += torch.sum((predicted==1)[labels==1])\n",
    "\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    specificity = TN/(TN+FP)\n",
    "    print(f'Sensitivity: {sensitivity*100} %')\n",
    "    print(f'Specificity: {specificity*100} %')\n",
    "    print()\n",
    "    print('         | {classes[0]} | {classes[1]} ')\n",
    "    print('---------|---------|----------')\n",
    "    print(f'negative |   {int(TP)}   |   {int(FP)}   ')\n",
    "    print(f'positive |   {int(FN)}   |   {int(TN)}   ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = test_set.subject*(test_set.groups*2-1)\n",
    "groups = torch.unique(subjects)\n",
    "groups[groups<0]=0\n",
    "groups[groups>0]=1\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "classes = ['CONTROL', 'PATIENT']\n",
    "\n",
    "with torch.no_grad():\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    \n",
    "    prev_subject = None\n",
    "    \n",
    "    predicted = []\n",
    "    \n",
    "    for i,(input_rawdata, input_scanpath, labels) in enumerate(test_loader):\n",
    "        \n",
    "        input_rawdata = input_rawdata[:,:,1:].to(device)\n",
    "        input_scanpath = (input_scanpath/128-1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        if prev_subject == subjects [i]:\n",
    "            outputs += model(input_rawdata, input_scanpath)\n",
    "        else:\n",
    "            predicted += [torch.max(outputs, 1)[1]]\n",
    "            \n",
    "            outputs = model(input_rawdata, input_scanpath)\n",
    "        \n",
    "        prev_subject = subjects[i]\n",
    "\n",
    "    predicted += [torch.max(outputs, 1)[1]]\n",
    "    predicted = torch.tensor(predicted[1:], dtype=torch.int64)\n",
    "        \n",
    "    TP += torch.sum((predicted==0)[groups == 0])\n",
    "    FN += torch.sum((predicted==1)[groups == 0])\n",
    "    FP += torch.sum((predicted==0)[groups == 1])\n",
    "    TN += torch.sum((predicted==1)[groups == 1])\n",
    "\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    specificity = TN/(TN+FP)\n",
    "    print(f'Sensitivity: {sensitivity*100} %')\n",
    "    print(f'Specificity: {specificity*100} %')\n",
    "    print()\n",
    "    print('         | Healthy |   Sick   ')\n",
    "    print('---------|---------|----------')\n",
    "    print(f'negative |   {int(TP)}   |   {int(FP)}   ')\n",
    "    print(f'positive |   {int(FN)}   |   {int(TN)}   ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
