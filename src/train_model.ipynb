{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.2.2+cu121\n",
      "Cuda Version: 12.1 \n",
      "\n",
      "Available devices:\n",
      "\t NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "\t\tMultiprocessor Count: 20\n",
      "\t\tTotal Memory: 3693.875 MB\n",
      "\n",
      " cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import tensorflow\n",
    "import torch\n",
    "import torch.version\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "torch.version.__version__\n",
    "print('PyTorch Version:',torch.version.__version__)\n",
    "print('Cuda Version:',torch.version.cuda,'\\n')\n",
    "\n",
    "print('Available devices:')\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print('\\t',torch.cuda.get_device_properties(i).name)\n",
    "   print('\\t\\tMultiprocessor Count:',torch.cuda.get_device_properties(i).multi_processor_count)\n",
    "   print('\\t\\tTotal Memory:',torch.cuda.get_device_properties(i).total_memory/1024/1024, 'MB')\n",
    "   \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('\\n',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage import transform\n",
    "import pickle\n",
    "from data_tools import *\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = '../datasets/VTNet/'\n",
    "\n",
    "with open(f'{DATASETS_DIR}trainset_vtnet.pkl', 'rb') as file:\n",
    "    train_set = pickle.load(file)\n",
    "\n",
    "with open(f'{DATASETS_DIR}valset_vtnet.pkl', 'rb') as file:\n",
    "    val_set = pickle.load(file)\n",
    "\n",
    "with open(f'{DATASETS_DIR}testset_vtnet.pkl', 'rb') as file:\n",
    "    test_set = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VETNet(nn.Module):\n",
    "    def __init__(self, timeseries_size ,scanpath_size):\n",
    "        super(VETNet, self).__init__()\n",
    "        self.scanpath_layer_conv1 = nn.Conv2d(scanpath_size[0],16,kernel_size=5,padding='same')\n",
    "        self.scanpath_layer_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.scanpath_layer_conv2 = nn.Conv2d(16,6,kernel_size=5,padding='same')\n",
    "        self.scanpath_layer_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.scanpath_layer_flatten = nn.Flatten()\n",
    "        self.scanpath_layer_linear = nn.Linear(scanpath_size[1]*scanpath_size[2]*6//16,50)\n",
    "        # self.scanpath_layer_relu = nn.LeakyReLU()\n",
    "        \n",
    "        self.timeseries_layer_attention = nn.MultiheadAttention(timeseries_size[-1]-1,1, batch_first=True)\n",
    "        self.timeseries_layer_gru = nn.GRU(input_size=timeseries_size[-1]-1, hidden_size=256, batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(306,20),\n",
    "            #nn.LeakyReLU(),\n",
    "            nn.Linear(20,2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_timeseries, x_scanpath):\n",
    "        x_scanpath = self.scanpath_layer_conv1(x_scanpath)\n",
    "        x_scanpath = self.scanpath_layer_pool1(x_scanpath)\n",
    "        x_scanpath = self.scanpath_layer_conv2(x_scanpath)\n",
    "        x_scanpath = self.scanpath_layer_pool2(x_scanpath)\n",
    "        x_scanpath = self.scanpath_layer_flatten(x_scanpath)\n",
    "        x_scanpath = self.scanpath_layer_linear(x_scanpath)\n",
    "        # x_scanpath = self.scanpath_layer_relu(x_scanpath)\n",
    "        \n",
    "        x_timeseries,_ = self.timeseries_layer_attention(x_timeseries,x_timeseries,x_timeseries)\n",
    "        _ ,x_timeseries = self.timeseries_layer_gru(x_timeseries)\n",
    "        \n",
    "        x = torch.cat((torch.squeeze(x_timeseries,0),x_scanpath),dim=1)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience: int, mode: str, minimum_delta = 0.0):\n",
    "        assert mode in {'min', 'max'}, \"mode has to be 'min' or 'max'\"\n",
    "        self.minimum_delta = minimum_delta\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.tracking = torch.inf if mode == 'min' else -torch.inf\n",
    "        self.mode = 1 if mode=='max' else -1\n",
    "        self.best_model = None\n",
    "        \n",
    "    def check(self, value, model: nn.Module):\n",
    "        if self.mode*(value-self.tracking) <= self.minimum_delta:\n",
    "            self.counter+=1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            self.tracking = value\n",
    "            self.best_model = deepcopy(model.state_dict())\n",
    "            \n",
    "        if self.counter == self.patience:\n",
    "            model.load_state_dict(self.best_model)\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "\n",
    "class ReduceLROnPlateau():\n",
    "    def __init__(self, patience: int, rate: float, mode: str, minimum_lr = 0.0, minimum_delta = 0.0):\n",
    "        assert rate<=1 and rate>0, \"rate as to be a number between 0 and 1\"\n",
    "        assert mode in {'min', 'max'}, \"mode has to be 'min' or 'max'\"\n",
    "\n",
    "        self.minimum_delta = minimum_delta\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.rate = rate\n",
    "        self.minimum_lr = minimum_lr\n",
    "        self.tracking = torch.inf if mode == 'min' else -torch.inf\n",
    "        self.mode = 1 if mode=='max' else -1\n",
    "        self.best_model = None\n",
    "        \n",
    "    def check(self, value, optimizer, model):\n",
    "        if self.mode*(value-self.tracking) <= self.minimum_delta:\n",
    "            self.counter+=1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            self.tracking = value\n",
    "            self.best_model = deepcopy(model.state_dict())\n",
    "            \n",
    "        if self.counter == self.patience:\n",
    "            for i in range(len(optimizer.param_groups)):\n",
    "                optimizer.param_groups[i]['lr'] = max(self.rate*optimizer.param_groups[i]['lr'], self.minimum_lr)\n",
    "            model.load_state_dict(self.best_model)\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VETNet(\n",
       "  (scanpath_layer_conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "  (scanpath_layer_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (scanpath_layer_conv2): Conv2d(16, 6, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "  (scanpath_layer_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (scanpath_layer_flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (scanpath_layer_linear): Linear(in_features=98304, out_features=50, bias=True)\n",
       "  (timeseries_layer_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (timeseries_layer_gru): GRU(6, 256, batch_first=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=306, out_features=20, bias=True)\n",
       "    (1): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VETNet(timeseries_size=train_set[0][0].shape, scanpath_size=train_set[0][1].shape).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "val_criterion = nn.CrossEntropyLoss()\n",
    "lr_tracker = ReduceLROnPlateau(5, 0.5, mode='min', minimum_lr=1e-5)\n",
    "earlystop_tracker = EarlyStopping(10, mode='min')\n",
    "\n",
    "trainloader = DataLoader(train_set, batch_size=batchsize, shuffle=True)\n",
    "valloader = DataLoader(val_set, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 144/144 [00:17<00:00,  8.08batch/s, loss=99.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0216, Validation Loss:  0.0291, Learning Rate:  0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 144/144 [00:18<00:00,  7.87batch/s, loss=88.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0193, Validation Loss:  0.0287, Learning Rate:  0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 144/144 [00:17<00:00,  8.12batch/s, loss=81]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0176, Validation Loss:  0.0309, Learning Rate:  0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 144/144 [00:17<00:00,  8.36batch/s, loss=68.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0150, Validation Loss:  0.0372, Learning Rate:  0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 144/144 [00:17<00:00,  8.31batch/s, loss=53.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0116, Validation Loss:  0.0457, Learning Rate:  0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 144/144 [00:17<00:00,  8.37batch/s, loss=35.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0078, Validation Loss:  0.0524, Learning Rate:  0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 144/144 [00:17<00:00,  8.27batch/s, loss=23.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0051, Validation Loss:  0.0635, Learning Rate:  0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 144/144 [00:17<00:00,  8.34batch/s, loss=79.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0174, Validation Loss:  0.0307, Learning Rate:  0.00005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 144/144 [00:17<00:00,  8.34batch/s, loss=71.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0156, Validation Loss:  0.0338, Learning Rate:  0.00005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 144/144 [00:17<00:00,  8.31batch/s, loss=62.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0136, Validation Loss:  0.0339, Learning Rate:  0.00005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 144/144 [00:17<00:00,  8.32batch/s, loss=52.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0114, Validation Loss:  0.0394, Learning Rate:  0.00005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 144/144 [00:17<00:00,  8.17batch/s, loss=40.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Training Loss (final):  0.0089, Validation Loss:  0.0462, Learning Rate:  0.00005\n"
     ]
    }
   ],
   "source": [
    "running_loss = []\n",
    "val_running_loss = []\n",
    "for epoch in range(1,101):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss += [0.0]\n",
    "    val_running_loss += [0.0]\n",
    "    \n",
    "    with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "        for input_rawdata, input_scanpath, labels in tepoch:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            input_rawdata = input_rawdata[:,:,1:].to(device)\n",
    "            input_scanpath = (input_scanpath/128-1).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(input_rawdata, input_scanpath)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss[-1] += loss.item()\n",
    "            \n",
    "            tepoch.set_postfix(loss=running_loss[-1])\n",
    "    \n",
    "        for val_rawdata, val_scanpath, val_labels in valloader:\n",
    "            val_rawdata = val_rawdata.to(device)\n",
    "            val_scanpath = (val_scanpath/128-1).to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            \n",
    "            val_outputs = model(val_rawdata[:,:,1:], val_scanpath)\n",
    "            val_loss = val_criterion(val_outputs, val_labels)\n",
    "            val_running_loss[-1] += val_loss.item()\n",
    "        \n",
    "        print(f\"\\t Training Loss (final): {running_loss[-1]/len(train_set): .4f}, Validation Loss: {val_running_loss[-1]/len(val_set): .4f}, Learning Rate: {optimizer.param_groups[-1]['lr']: .5f}\")\n",
    "        \n",
    "        lr_tracker.check(value=val_running_loss[-1], optimizer=optimizer, model=model)\n",
    "        \n",
    "        if earlystop_tracker.check(value=val_running_loss[-1], model=model):\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 53.125 %\n",
      "Specificity: 61.224491119384766 %\n",
      "\n",
      "         | Healthy |   Sick   \n",
      "---------|---------|----------\n",
      "negative |   136   |   133   \n",
      "positive |   120   |   210   \n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=batchsize, shuffle=True)\n",
    "classes = ['CONTROL', 'PATIENT']\n",
    "\n",
    "with torch.no_grad():\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    \n",
    "    for input_rawdata, input_scanpath, labels in test_loader:\n",
    "        \n",
    "        input_rawdata = input_rawdata[:,:,1:].to(device)\n",
    "        input_scanpath = (input_scanpath/128-1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(input_rawdata, input_scanpath)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        TP += torch.sum((predicted==0)[labels==0])\n",
    "        FN += torch.sum((predicted==1)[labels==0])\n",
    "        FP += torch.sum((predicted==0)[labels==1])\n",
    "        TN += torch.sum((predicted==1)[labels==1])\n",
    "\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    specificity = TN/(TN+FP)\n",
    "    print(f'Sensitivity: {sensitivity*100} %')\n",
    "    print(f'Specificity: {specificity*100} %')\n",
    "    print()\n",
    "    print('         | Healthy |   Sick   ')\n",
    "    print('---------|---------|----------')\n",
    "    print(f'negative |   {int(TP)}   |   {int(FP)}   ')\n",
    "    print(f'positive |   {int(FN)}   |   {int(TN)}   ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
